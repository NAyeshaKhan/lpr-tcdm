GitHub link to project: https://github.com/NAyeshaKhan/lpr-tcdm
Checkpoints have been added to the zip file as they are too big to include in the GitHub repository. Place it in the folder stisr-tcdm/ckpt. The checkpoint folder can be downloaded from here: https://drive.google.com/file/d/1bGdMXdlaDrxp2AhaZR6AuEZeIVuLlcpJ/view?usp=sharing, but has also been uploaded to Moodle. The directory should be as follows:
stisr-tcdm
|-- ckpt
    |-- super_resolver
        |--ema_0.9999_100000.pt
    |-- synthesizer
        |--ema_0.9999_150000.pt
    |-- degrader
        |--ema_0.9999_150000.pt

To get started, install the required python packages using the following command:
pip install -e .

The instructions below are the general steps needed to run the whole framework from start to finish. To run the super-resolver on the TextZoom and RLPR datasets, you will only need to run the following commands:
	bash run_super_resolver.sh (for inference on the TextZoom)
	bash run_sr_rlpr.sh (for inference on the RLPR)
To evaluate the generated results, run the following commands:
	python evaluate.py (for evaluating the super-resolved TextZoom images)
	python eval_rlpr.py (for evaluating the super-resolved RLPR images)

Dataset
----------
Download the TextZoom dataset from: https://github.com/JasonBoy1/TextZoom
The structure of dataset directory is
dataset
`-- TextZoom
    |-- test
    |   |-- easy
    |   |   |-- data.mdb
    |   |   `-- lock.mdb
    |   |-- hard
    |   |   |-- data.mdb
    |   |   `-- lock.mdb
    |   `-- medium
    |       |-- data.mdb
    |       `-- lock.mdb
    |-- train1
    |   |-- data.mdb
    |   `-- lock.mdb
    `-- train2
        |-- data.mdb
        `-- lock.mdb

To train the Synthesizer, the preprocessed STR dataset is required in addition to the TextZoom dataset. Download the preprocessed STR dataset from: https://github.com/ku21fan/STR-Fewer-Labels.
Download the alternative dataset: RLPR from here: https://data.mendeley.com/datasets/4rs5wpvckz/1
As the images needed for running the super-resolver have already been generated as .npz files, it is not necessary to download the above datasets for the purpose of this project.

Pretrained recognizers
----------------------
Download pretrained recognizers (CRNN, ASTER, MORAN).

CRNN: https://github.com/meijieru/crnn.pytorch
ASTER: https://github.com/ayumiymk/aster.pytorch  
MORAN: https://github.com/Canjie-Luo/MORAN_v2

The pretrained weights can be found in /text_recognition/ckpt

Training
----------
1. To train GT-DiMSS on the TextZoom dataset, run the script via: bash train_gt_dimss_textzoom.sh
To generate SR images from the LR images of TextZoom with a trained GT-DiMSS, run the script via: bash eval_gt_dimss_textzoom.sh
For this project, the GT-DiMSS has been pretrained, and the weights have ben saved in "stisr-tcdm/ckpt/super_resolver".
The DiMSS model is only used for ablation studies in the paper, so while it has been trained, you will not need it to run this project.

2. LR-HR Paired Text Image Synthesis
a) Training Synthesizer and Degrader
To perform the preprocessing for the Synthesizer training, run the script via: python preprocessing_STR.py
When the preprocessing is complete, preprocessed text images and the corresponding text labels are placed in "dataset/STR/img" and "dataset/STR/word", respectively.
To train Synthesizer, run the script via: bash train_synthesizer.sh
To perform the preprocessing for the generated text images, run the script via: python postprocessing_text_images.py
The postprocessed text images are placed in "./diff_samples/mr_samples/postprocessed".

The degrader is trained on the TextZoom dataset. To train the Degrader, run the script via: bash train_degrader.sh

For the purposes of this project, the synthesizer and degrader have already been trained, and the pretrained model weights can be found in /ckpt/synthesizer and stisr-tcdm/ckpt/degrader, so there is no need to download the dataset or train the synthesizer and degrader from scratch. 

b) Running models
To run the Synthesizer, run the script via: bash run_synthesizer.sh. The generated text images and the corresponding text labels are placed in "./diff_samples/mr_samples".
To run the Degrader, run the script via: bash run_degrader.sh. The generated LR text images are placed in "./diff_samples/lr_samples".

The generated LR, MR, and HR samples can be found in stisr-tcdm/diff_samples.

3. Generating LR and HR text images
To run Super-resolver on the TextZoom dataset, run the script via: bash run_super_resolver.sh. This originally used the MR images in "/diff_samples/mr_samples", but the model in the project has used the resized MR samples in "/diff_samples/mr_samples/resized-64x128"
To run Super-resolver on the RLPR dataset, run the script via: bash run_sr_rlpr.sh. 
To evaluate the results for the super-resolved TextZoom images, you will need the .npz files generated by the GT-DiMSS model via running the script: run bash eval_gt_dimss_textzoom.sh. For this project, the MR image samples have already been placed in "/diff_samples/textzoom" with the subset difficulty level, such as easy_sr_samples_gt_dimss.npz. Then, run evaluate.py
To evaluate the results for the super-resolved RLPR images, run eval_rlpr.py. You will need the .npz file RLPR_HR_generated.npz placed in "/diff_samples/hr_samples".

To visualise the .npz files generated by the super-resolver model as images, you can run: python vis_images.py. This will save the images to the folder "saved_images". An equivalent file in MATLAB can be found in visualize_npz.m and readNPZ.m.

A video of running the evaluations files on the TextZoom and RLPR datasets has been added to the .zip file.

